---
layout: post
title: 对<智能增长的瓶颈>一文的一些看法
description: 阅读完尤洋的微信文章后有感而发
lang: zh
tags: [misc]
---


<style>
.highlight-left {margin-left: 0}
</style>

> Intelligence is the ability to adapt to change --Stephen Hawking

尤洋是我在谷歌大脑任职期间有过短暂交集的一位朋友。今天读到他写的微信公众号文章：智能增长的瓶颈，不禁有一些感想不吐不快。原文在此处：[智能增长的瓶颈](https://mp.weixin.qq.com/s/BP5wWOtqQJzUDOztF9zG5w)

首先智能的核心在我看来就是霍金说的这句话：对于变化的适应能力。这在我看来无疑是一个更本质，更优雅的定义。并且也能完全包含预测和创作的两个场景：预测是基于历史与现状，对未来状态的推断，本质是应对外部不确定性的适应性策略。创作是生成新的、合理的结构（文本、方案、艺术品），本质是为满足新需求或解决新问题而进行的适应性创造。这两者都是智能系统在适应复杂、动态环境时，所展现出的高级行为。而对变化的适应性才是智能的根本目的和核心功能。这里尤洋对智能的定义我觉得是将表现形式和手段当作了本质。

从这个角度出发去谈智能发展的瓶颈，当然也因此过于简单的归因于算力效率的不足。而忽略了范式层面的一些其他瓶颈。

当然，我完全认可他对Transformer并行计算的论断，我同样也认可使用从能源到智能的转换率作为标准之一衡量智能的进展步伐。然而就在他试图建立智能本质的衡量标准时，他犯了第一个错误：比GPT快5倍，或者小10倍的模型只是省钱的技术，只是压缩算法而不是智能突破，只有在同等巨大算力下表现更好，才是真理。

此处对于效率与智能关系的割裂，是违反信息论和现代AI发展史的。效率本身就是智能最高的体现。我相信Marcus Hutter一定会同意我的说法 :-P 如果明天你提出一个架构，用20%的参数量达到了GPT-5的效果，这绝对是智能本质的巨大突破，而不仅仅是商业落地问题。实际上，物理学和计算机科学普遍认为，理解一个事物的标志，就是看你能用多短的代码(最小描述长度,MDL)去描述它。一个使用20%算力就达到同等水平的模型，说明它具有更好的归纳偏置(Inductive Bias)，这里"少"就是"精品",而"精"就是"智"。

另外，Scaling Law是具有双向性的。这就是说，如果我发现一个20%算力下就能打平Transformer的架构，科研的下一步一定是会立刻把该模型扩大五倍，把剩下的80%算力也充分利用。那么此时，根据缩放定律，这个新的模型将会远远碾压GPT-5。所以节约算力的技术本质上是在提升智能上限。

最后，浮点计算次数才是算力最基本最本质的计量单位。即使我也是曾经从事HPC的人，我仍然不能同意这句话在AI语境里的意义。

FLOPs不等于思考：众所周知，人脑的功耗大约是20瓦，算力换算成FLOPs远低于现在的超算集群，且人脑根本不使用FP进行操作。但是人脑创造了当今世界的一切奇迹。

不同架构的FLOPS产生的价值不同：Mamba/RWKV等架构下的FLOPS产生的价值与Transformer的FLOPS价值不同。system 2 (推理时计算)模型在推理阶段用于搜索验证的FLOPS比预训练时的FLOPS对某些特定任务价值更大。

姑且让我这样定义智能的未来发展趋势：在单位FLOPs内产生最大的熵减，或者更直白的叫法：智能密度。单纯追求堆砌FLOPs肯定只是在比谁家发电多，而不是谁家算法好。

Smart不等于Big。不过更轻盈更高效往往是通往更强大的必经之路。虽然商业化关注如何在小算力下达到同等效果（降本），但科学界更关注这个高效架构在**扩大规模（Scale Up）**后，是否能突破现有模型的智能天花板。效率的提升，往往意味着掌握了更本质的规律。

接下去，我认为文中最大且最危险的误判就是关于使用高精度FP64提升职能的谬误。作者认为提升精度，甚至于FP64的回归可能是提升智能的瓶颈突破口，并将数值计算的精确性等同于职能的可靠性。

然而，现代深度学习最基本最普遍的理论就是：高维数据分布在一个低维流形上。而神经网络的本质就是学习这个流形的拓扑结构，而非拟合每一个微小的数值扰动。智能的体现恰恰在于抗噪性，即在低精度、高噪声的输入下仍然能输出正确决策。实际上低精度让算力效率和存储效率双双提升，再使用缩放定律来提升智能瓶颈恰恰才是如今的行业趋势。科学计算的本质在于求解高维的微分方程(比如对于天气预测，求解Navier-Stokes方程，对于材料分析，求解薛定谔方程)。使用FP64的原因在于迭代的科学计算过程对于累计误差非常敏感，在非线性系统中，微小的误差会在几千次迭代后指数级放大，导致最终结果完全错误。而AI，或者智能计算的本质在于统计性的模糊正确。本质是寻找高维空间中的概率分布和决策边界。神经网络权重和激活值本身就是从嘈杂的数据中学来的近似值。事实上，低精度引入的随机噪声反而能防止过拟合，迫使模型更加鲁棒。从去年到今年，量化的研究已经将神经网络权重压缩到ternary级别的1.58比特。这完全与走向FP64相反。

至于天气预报和地震预测对比的例子，地震预测之核心难点不是维度爆炸，而是其地下无传感器的黑盒状态。比如我们无法知道数十公里下的岩石应力和摩擦系数等关键参数。对比天气，这类数据的获取显然容易得多。所以并非计算精度不够，而是输入数据缺失。我想，AlphaFold的成功当然并非建立在FP64精度计算上，而是其神经网络架构和基于海量蛋白质数据的学习过程。AGI不必成为一个超级物理模拟器，而且恰恰不应该成为一个超级物理模拟器。

最后，不可免俗的，作者需要回到Rich Sutton的Bitter Lesson来结束文章，做一些提升。然而，通用方法不等于简单懒惰的Scaling。Sutton这里原文的核心在于不要试图把人类的领域知识硬编码到代码里，减少这样的人为归纳偏置，而构建一个能利用算力自动学习这些知识的系统。事实上，这一论断甚至无关算力：从CNN取代SIFT/HOG，到Transformer取代RNN/LSTM等，都是Sutton论断的胜利。真正苦涩的教训的信徒，应该去寻找下一代比Transformer更通用，更少人为约束的架构。

这篇文章在硬件理解和并行计算的直观解释上非常出色，但是作为一篇展望2026年的文章，我很遗憾的预测作者希望发生的很多事情都不会发生甚至会像他预想的方向相反的方向发生。我认为，这几件事情才是2026年及以后智能计算的趋势：
1. 推理时搜索与规划和Agent Swarm
2. 数据合成与自我对弈解决数据枯竭问题
3. 稀疏和低精度带来更高算力效率和计算范式革命
4. 其他新架构和新学习范式的革命


