---
layout: post
title: GPU Programming - Two-Phase Strategy
description: This article is heavily inspired by Sean Baxter's articles in moderngpu 1.0 wiki (https://moderngpu.github.io/performance.html)
lang: en
---

Programming GPU is difficult. Although recent years, deep learning has brought GPU computing to almost every framework and library, most of them are either 1) naively implemented embarrassingly parallel tasks, or 2) wrappers of out-of-box libraries provided by NVIDIA such as cuBLAS, cuDNN, CUB, and Thrust. On the other hand, the GPU has high memory bandwidth and a well-designed latency-hiding architecture which is well suited for fine-grained manipulation of data. The goal for this series of articles/code is to be flexible enough for users to dive into the details of algorithm designs that can expose parallelism in all kinds of problems (including irregular ones), while maintaining a certain level of abstraction so that the code is still decomposible and has good readability.

To achieve this goal, we follow the two-phase strategy proposed by Sean Baxter in his moderngpu library. Two-phase strategy solves problems by breaking solutions into tow distinct phases:
- Find a coarse-grained partitioning of the problem that exactly load-balances work over each thread. Scheduling—the exercise of mapping work items to CTAs (cooperative thread arrays) and threads on the GPU—is handled in this phase.
- Execute simple, work-efficient, sequential logic that solves the problem. Because scheduling is part of the partitioning phase, this sequential phase runs embarrassingly parallel.

The benefits of this is obvious: by decoupling partitioning from the work logic, we improve modularity: For a new primitive that share the same partitioning strategy, we reuse a partitioning strategy and only write the work logic for that specific primitive. For a new type of problems that needs new partitioning strategy, we add a new one (with probably only a few modification on an existing one, thanks to the power of modularity and hierarchy).

Like we mentioned in the previous article, performance tuning is simple. Each thread processes VT (values per thread) items. Increasing VT assigns more work to each thread, amortizing the cost of partitioning, which is constant per thread. Although this increases work-efficiency, it decreases parallelism and the GPU's ability to hide latency.

__MergeSort: An Example of Two-phase Strategy__
A CPU sequential merge implementation takes two sorted inputs and loops over each output. During each iteration, two inputs are compared and the smaller one is emitted. The implementation is simple and work-efficient (O(N)). Because it is totally sequential, there is no real consideration given to scheduling or decomposition.

```C++
template<typename T, typename Comp>
void CPUMerge(const T* a, int a_count, const T* b, int b_count, T* dest, Comp comp) {
    int count = a_count + b_count;
    int ai = 0, bi = 0;
    for (int i = 0; i < count; ++i) {
        bool p;
        if (bi >= b_count) p = true;
        else if (ai >= a_count) p = false;
        else p = !comp(b[bi], a[ai]);

        dest[i] = p ? a[ai++] : b[bi++];
    }
}
```

Now consider the first attempt at parallel merge. We need two kernels: _ParallelMergeA_, which assigns one thread to each element in A, binary searches for the lower-bound in B, and output A keys to the destination; and _ParallelMergeB_, which assigns one thread to each element in B, binary searches for the upper-bound in A, and output B keys to the destination.

```C++
template<bounds_t bounds, typename keys_it, typename int_t, typename key_t, typename comp_t>
__device__ int_t binary_search(keys_it keys, int_t count, key_t key, comp_t comp) {
    int_t begin = 0;
    int_t end = count;
    while (begin < end) {
        int_t mid = (begin + end) / 2;
        key_t key2 = keys[mid];
        bool pred = (bounds_upper == bounds) ?
            !comp(key, key2) :
            comp(key2, key);
        if (pred) begin = mid + 1;
        else end = mid;
    }
    return begin;
}
```

```C++
template<int NT, typename input1_t, typename input2_t, typename output_t, typename comp_t>
__global__ void ParallelMergeA(input1_t a_global, int a_count, intput2_t b_global, int b_count, output_t dest_global, comp_t comp) {
    typedef typename std::iterator_traits<input1_t>::value_type T;

    int gid = threadIdx.x + NT * blockIdx.x;
    if (gid < a_count) {
        T a_key = a_global[gid];
        int lb = binary_search<bounds_lower>(b_global, b_count, a_key, comp);
        dest_global[gid + lb] = a_key;
    }
}

template<int NT, typename input1_t, typename input2_t, typename output_t, typename comp_t>
__global__ void ParallelMergeB(input1_t a_global, int a_count, intput2_t b_global, int b_count, output_t dest_global, comp_t comp) {
    typedef typename std::iterator_traits<input2_t>::value_type T;

    int gid = threadIdx.x + NT * blockIdx.x;
    if (gid < b_count) {
        T b_key = b_global[gid];
        int ub = binary_search<bounds_upper>(a_global, a_count, b_key, comp);
        dest_global[gid + ub] = b_key;
    }
}
```

This implementation is highly concurrent, but it is also highly inefficient. The O(N)-efficiency sequantial code is now O(NlogN), as each output requires a binary search over the input. The code looks nothing like its sequential version either. Because the new implementation fused scheduling logic into the problem-solving logic. Here is a code that is difficult to optimize and more difficult to extend.

Using two-phase strategy, we propose that the correct way to think about mergesort in the GPU context is to first partition both A and B into several small chunks, where each chunk A and chunk B sum up to an equal number K (to gain load-balancing). Then within each chunk A and chunk B, we could use the same sequential merge work logic. For different GPUs, K can be tuned to gain the best performance. Here the partitioning strategy we describe is called [Merge Path](https://www.academia.edu/10626697/Merge_Path_-Parallel_Merging_Made_Simple), and can be applied to several GPU primitives to do load-balancing workload mapping.
![MergePath Partitioning](/public/images/mp.png){:height="50%" width="50%"}
The illustration shows this partitioning strategy: the green merge path is equally divided by 8 diagonal lines. The following code shows how to find the beginning indices for each local chunk A and chunk B. We will show detailed analysis in our following articles.
```C++
// For N = a_count + b_count, diag = i * K (i = 0, div_up(N, chunk_num) - 1)
template<bounds_t bounds, typename it1_t, typename it2_t, typename comp_t>
__device__ int MergePath(it1_t a, int a_count, it2_t b, int b_count, int diag, comp_t comp) {
    typedef typename std::iterator_traits<it1_t>::value_type T;
    int begin = max(0, diag - b_count);
    int end = min(diag, a_count);

    while (begin < end) {
        int mid = (begin + end) >> 1;
        T a_key = a[mid];
        T b_key = b[diag - 1 - mid];
        bool pred = (bounds_upper == bounds) ?
            comp(a_key, b_key) :
            !comp(b_key, a_key);
        if (pred) begin = mid + 1;
        else end = mid;
    }
    return begin;
}
```

When we know the beginning indices for each local chunk A and chunk B, we can easily write serial merge code just like the sequantial version:
```C++
// This is a simplified version that only shows the algorithm
// but ignores proper output.
template<int VT, bool range_check, typename T, typename comp_t>
__device__ void SerialMerge(const T* keys_shared, int a_begin, int a_end, int b_begin, int b_end, T* results, int* indices, comp_t comp) {
    T a_key = keys_shared[a_begin];
    T b_key = keys_shared[b_begin];

    #pragma unroll
    for (int i = 0; i < VT; ++i) {
        bool p;
        if (range_check)
            p = (b_begin >= b_end) || ((a_begin < a_end) && !comp(b_key, a_key));
        else
            p = !comp(b_key, a_key);
        
        results[i] = p ? a_key : b_key;
        indices[i] = p ? a_begin : b_begin;

        if (p) a_key = keys_shared[++a_begin];
        else b_key = keys_shared[++b_begin];
    }
    __syncthreads();
}
```

Note that this work logic implementation achieves work-efficiency via two aspects:
- It processes VT inputs for each binary search;
- It unrolls the serial merge loop and stores intermediates to register to conserve shared memory (improve occupancy).

Here we quote the conclusion from moderngpu 1.0's Introduction: "Kernels written using the two-phase decomposition are more involved than direct solutions like the naive parallel merge, but are more efficient, easier to optimize, and more flexible; they easily accommodate algorithmic changes to solve related problems."

Our next post will give a full analysis on how to write high performance implementations for(perhaps) the most two popular GPU kernels: Scan and Reduce.